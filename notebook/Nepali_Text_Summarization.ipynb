{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ck_bt4eZ7gyy",
        "outputId": "baebe9f9-e45b-4ff5-cb02-e848c2da2124"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import numpy as np\n",
        "import nltk\n",
        "from collections import Counter\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "import math\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "#ps = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"नेपाल एक अद्भुत देश हो।\",\n",
        "        \"यो हिमालयको छायामा बस्ने जग्गा हो जसले प्राकृतिक सौन्दर्यलाई चिनारी दिन्छ।\",\n",
        "        \"पहाडी तराई र हिमालयको मेलमिलापले यसलाई विशेष बनाउँछ।\",\n",
        "        \"प्रकृतिक सौन्दर्य, विविधता, र ऐतिहासिक समृद्धिले नेपाललाई विश्वभरी प्रसिद्ध बनाएको छ।\",\n",
        "        \"यो देश भिन्न जाति, भाषा, र संस्कृतिको संग्रहणभरिको छनौटमा बिराजमान छ।\",\n",
        "        \"यहाँका मानवतावादी भावनाहरू विश्वसामान्य रूपमा स्वागत गरिन्छ।\",\n",
        "        \"समृद्धि, सहजता, र सामाजिक न्यायलाई अपेक्षा गर्दै नेपाली लोकसेवा गर्दछन्।\"]"
      ],
      "metadata": {
        "id": "IWoAyB3vGXXE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocessing(sentences):\n",
        "    \"\"\"\n",
        "    Preprocessing text to remove unnecessary words.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('nepali'))\n",
        "\n",
        "    clean_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # Remove punctuation\n",
        "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
        "        sentence_no_punct = sentence.translate(translator)\n",
        "\n",
        "        # Tokenize sentence into words\n",
        "        words = sentence_no_punct.split()\n",
        "\n",
        "        # Remove stop words\n",
        "        words_no_stop = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "        # Join words to reconstruct sentence\n",
        "        clean_sentence = ' '.join(words_no_stop)\n",
        "        clean_sentences.append(clean_sentence)\n",
        "\n",
        "    return clean_sentences"
      ],
      "metadata": {
        "id": "Kn12_jJa9TUX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tf_matrix(sentences):\n",
        "  preprocessed_sentences = text_preprocessing(sentences)\n",
        "  tf_matrix = list()\n",
        "\n",
        "  for sentence in preprocessed_sentences:\n",
        "    words = sentence.split()\n",
        "    total_words = len(words)\n",
        "    words_frequency = {}\n",
        "\n",
        "    for word in words:\n",
        "      if word not in words_frequency:\n",
        "        words_frequency[word] = 0\n",
        "      words_frequency[word] += 1/total_words\n",
        "\n",
        "    tf_matrix.append(words_frequency)\n",
        "  return tf_matrix"
      ],
      "metadata": {
        "id": "r4q05UIUI3kO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_idf_matrix(tf_matrix):\n",
        "  unique_words = set(word for tf_dict in tf_matrix for word in tf_dict)\n",
        "\n",
        "  doc_frequency = {word:sum(word in tf_dict for tf_dict in tf_matrix) for word in unique_words}\n",
        "\n",
        "  total_docs = len(tf_matrix)\n",
        "  idf_matrix = {word:math.log(total_docs/(1+doc_freq)) for word, doc_freq in doc_frequency.items()}\n",
        "  return idf_matrix"
      ],
      "metadata": {
        "id": "DVtj66CyFmQ4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tfidf_matrix(sentences):\n",
        "  tf_mat = create_tf_matrix(sentences)\n",
        "  idf_mat = create_idf_matrix(tf_mat)\n",
        "\n",
        "  tfidf_matrix = list()\n",
        "\n",
        "  for tf_dict in tf_mat:\n",
        "    tfidf_dict = {}\n",
        "    for word, tf_score in tf_dict.items():\n",
        "      tfidf_dict[word] = tf_score * idf_mat.get(word,0)\n",
        "    tfidf_matrix.append(tfidf_dict)\n",
        "\n",
        "  return tfidf_matrix"
      ],
      "metadata": {
        "id": "BJX-aagFEZm3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_sentence_scores(tfidf_matrix):\n",
        "  sentence_scores = list()\n",
        "\n",
        "  for tfidf_dict in tfidf_matrix:\n",
        "    total_score = sum(tfidf_score for tfidf_score in tfidf_dict.values())\n",
        "    distinct_words = len(tfidf_dict)\n",
        "\n",
        "    if distinct_words != 0:\n",
        "      avg_score = total_score/distinct_words\n",
        "    else:\n",
        "      avg_score = 0\n",
        "    sentence_scores.append(avg_score)\n",
        "\n",
        "  return sentence_scores\n",
        "\n",
        "calculate_sentence_scores(create_tfidf_matrix(text))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ioVwa3AEphR",
        "outputId": "c2c734ce-eb64-4c61-c838-911974620ece"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-0.013862943611198915]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_average_score(sentence_scores):\n",
        "  total_score = sum(sentence_scores)\n",
        "  num_sentences = len(sentence_scores)\n",
        "\n",
        "  if num_sentences != 0:\n",
        "    average_score = total_score / num_sentences\n",
        "  else:\n",
        "    average_score = 0\n",
        "\n",
        "  return average_score\n",
        "\n",
        "calculate_average_score(calculate_sentence_scores(create_tfidf_matrix(text)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NhZikpZpE--x",
        "outputId": "aec7eca0-90e2-43ac-a3ca-a907971e8989"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-0.013862943611198915"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(sentences):\n",
        "  tfidf_matrix = create_tfidf_matrix(text)\n",
        "  sentence_scores = calculate_sentence_scores(create_tfidf_matrix(text))\n",
        "  average_score = calculate_average_score(sentence_scores)\n",
        "\n",
        "  threshold = average_score\n",
        "\n",
        "  summary_sentences = [sentences[i] for i, score in enumerate(sentence_scores) if score > threshold]\n",
        "  summary = ' '.join(summary_sentences)\n",
        "\n",
        "  return summary"
      ],
      "metadata": {
        "id": "mNK0CYOIU5Au"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_summary(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "FPgeuW6oVTAD",
        "outputId": "914794b2-f83a-4d1c-bd5d-fb2539ad6d05"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'नेपाल एक अद्भुत देश हो। पहाडी तराई र हिमालयको मेलमिलापले यसलाई विशेष बनाउँछ। यहाँका मानवतावादी भावनाहरू विश्वसामान्य रूपमा स्वागत गरिन्छ।'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "x4fexvqRXIJ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile summarizer.py\n",
        "\n",
        "import numpy as np\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "import math\n",
        "from dataclasses import dataclass\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "@dataclass\n",
        "class Nepali_summarizer:\n",
        "  text:list = None\n",
        "\n",
        "  def text_preprocessing(self,sentences):\n",
        "    \"\"\"\n",
        "    Preprocessing text to remove unnecessary words.\n",
        "    \"\"\"\n",
        "    stop_words = set(stopwords.words('nepali'))\n",
        "\n",
        "    clean_sentences = []\n",
        "    for sentence in sentences:\n",
        "        # Remove punctuation\n",
        "        translator = str.maketrans(\"\", \"\", string.punctuation)\n",
        "        sentence_no_punct = sentence.translate(translator)\n",
        "\n",
        "        # Tokenize sentence into words\n",
        "        words = sentence_no_punct.split()\n",
        "\n",
        "        # Remove stop words\n",
        "        words_no_stop = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "        # Join words to reconstruct sentence\n",
        "        clean_sentence = ' '.join(words_no_stop)\n",
        "        clean_sentences.append(clean_sentence)\n",
        "\n",
        "    return clean_sentences\n",
        "\n",
        "  def create_tf_matrix(self,sentences):\n",
        "    preprocessed_sentences = self.text_preprocessing(sentences)\n",
        "    tf_matrix = list()\n",
        "\n",
        "    for sentence in preprocessed_sentences:\n",
        "      words = sentence.split()\n",
        "      total_words = len(words)\n",
        "      words_frequency = {}\n",
        "\n",
        "      for word in words:\n",
        "        if word not in words_frequency:\n",
        "          words_frequency[word] = 0\n",
        "        words_frequency[word] += 1/total_words\n",
        "\n",
        "      tf_matrix.append(words_frequency)\n",
        "    return tf_matrix\n",
        "\n",
        "  def create_idf_matrix(self,tf_matrix):\n",
        "    unique_words = set(word for tf_dict in tf_matrix for word in tf_dict)\n",
        "\n",
        "    doc_frequency = {word:sum(word in tf_dict for tf_dict in tf_matrix) for word in unique_words}\n",
        "\n",
        "    total_docs = len(tf_matrix)\n",
        "    idf_matrix = {word:math.log(total_docs/(1+doc_freq)) for word, doc_freq in doc_frequency.items()}\n",
        "    return idf_matrix\n",
        "\n",
        "  def create_tfidf_matrix(self,sentences):\n",
        "    tf_mat = self.create_tf_matrix(sentences)\n",
        "    idf_mat = self.create_idf_matrix(tf_mat)\n",
        "\n",
        "    tfidf_matrix = list()\n",
        "\n",
        "    for tf_dict in tf_mat:\n",
        "      tfidf_dict = {}\n",
        "      for word, tf_score in tf_dict.items():\n",
        "        tfidf_dict[word] = tf_score * idf_mat.get(word,0)\n",
        "      tfidf_matrix.append(tfidf_dict)\n",
        "\n",
        "    return tfidf_matrix\n",
        "\n",
        "  def calculate_sentence_scores(self,tfidf_matrix):\n",
        "    sentence_scores = list()\n",
        "\n",
        "    for tfidf_dict in tfidf_matrix:\n",
        "      total_score = sum(tfidf_score for tfidf_score in tfidf_dict.values())\n",
        "      distinct_words = len(tfidf_dict)\n",
        "\n",
        "      if distinct_words != 0:\n",
        "        avg_score = total_score/distinct_words\n",
        "      else:\n",
        "        avg_score = 0\n",
        "      sentence_scores.append(avg_score)\n",
        "\n",
        "    return sentence_scores\n",
        "\n",
        "  def calculate_average_score(self, sentence_scores):\n",
        "    total_score = sum(sentence_scores)\n",
        "    num_sentences = len(sentence_scores)\n",
        "\n",
        "    if num_sentences != 0:\n",
        "      average_score = total_score / num_sentences\n",
        "    else:\n",
        "      average_score = 0\n",
        "\n",
        "    return average_score\n",
        "\n",
        "  def generate_summary(self,sentences):\n",
        "    tfidf_matrix = self.create_tfidf_matrix(text)\n",
        "    sentence_scores = self.calculate_sentence_scores(create_tfidf_matrix(text))\n",
        "    average_score = self.calculate_average_score(sentence_scores)\n",
        "\n",
        "    threshold = average_score\n",
        "\n",
        "    summary_sentences = [sentences[i] for i, score in enumerate(sentence_scores) if score > threshold]\n",
        "    summary = ' '.join(summary_sentences)\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lOAy7PY3XH-m",
        "outputId": "8f6843fb-a70a-495c-e3b0-d119e9343032"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing summarizer.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer = Nepali_summarizer()"
      ],
      "metadata": {
        "id": "uUPWuK7iaGQi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "summarizer.generate_summary(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7AFQCK_LaMXy",
        "outputId": "89e3bd7c-6fcd-42a1-90fa-e3083ea957b6"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'नेपाल एक अद्भुत देश हो। पहाडी तराई र हिमालयको मेलमिलापले यसलाई विशेष बनाउँछ। यहाँका मानवतावादी भावनाहरू विश्वसामान्य रूपमा स्वागत गरिन्छ।'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OIgmcT6naPdi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}